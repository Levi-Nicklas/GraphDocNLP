---
title: "Graph Kernels as Preprocessing for Unsupervised Learning: Two Case Studies"
author: "Levi C. Nicklas (B.S. Applied Mathematics)"
subtitle: "<html><div style='float:left'></div><hr color='#522A87' size=1px width=796px></html>"
#author: Levi Nicklas
date: Masters of Science in Computer Science (Data Science Track) Candidate #"`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ["poly.css", metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(here)
knitr::opts_chunk$set(echo = FALSE)
```



class: inverse, center, middle
name: Intro

# Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Introduction
## The Big Idea

One of the many methods in text mining and text visualization is to express the text as a bi-gram, **skip-gram**, or other n-gram graphs, but can these ideas be extended into any machine learning applications?

--

Through use of **graph kernels** two, or more, graphs can be compared to one another. This resulting measure of similarity then can be used in a variety of machine learning applications.

--

The application of interest here is to use the similarity measures to perform unsupervised learning in the form of **hierarchical clustering**.




---

# Introduction to the Data

Two data sets are use for all of the analyses, one coming from the National Highway Traffic Safety Administration ( [NHTSA](https://www.nhtsa.gov/research-data/special-crash-investigations-sci)) and one from the popular social media website [reddit.com](reddit.com/r/mentalhealth/). 

--

The NHTSA data are a series of special crash investigations (SCI) which are public ally available through a portal in PDF format. The SCI program provides the agency with detailed results from investigative specialists who detail and carefully document the incident of concern. $(N=48)$

--

The reddit data are public online forum discussion on the `r\MentalHealth` subreddit which satisfied query criteria. The use of reddit in this particular topic is critical, as reddit is well known for the platform, and its users' strong commitment to anonymous online interactions. This type of environment often leads to candid and honest discussion about personal problems. $(N=1000)$
---

# Introduction to the Data (cont.)
## NHTSA SCI Reports

- **Motivation:** These efforts and results support initiatives in Florida Polytechnic University's Advanced Mobility Institute (AMI) which focuses in the development of connected and fully autonomous vehicles. These reports are all on incidents which involved ambulances in the collision. The events all represent "edge cases" that are rare and need to be characterized properly for AMI's future success in developing robust autonomous vehicles.

- **Format and Collection:** the reports are all stored as PDF files which are all converted to XML and from which the text of concern is then parsed out for further processing and analysis.

```{r, out.width = "400px", fig.retina = 3, echo  = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://crashviewer.nhtsa.dot.gov/Images/NHTSA_Logo_w_tag.png")
```
---

# Introduction to the Data (cont.)
## Mental Health Subreddit

- **Motivation:** Previously published research work was focused on text mining within reddit subreddits focused on opiate addiction. The paper focused on clustering of posts/comments from the data collected from the subreddit. Using this type of data with new methods builds upon my existing work.

- **Format and Collection:** the thread text is collected with `{redditExtractor}` and then must undergo a cleaning and reformatting, as the tabular format is structured such that observational units are threads. 

```{r, out.width = "300px", fig.retina = 3, echo  = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://logos-download.com/wp-content/uploads/2016/06/Reddit_logo_full_1.png")
```

---

# Introduction to the Data (cont.)
## Comparison of Text

The datasets were chosen for the analyses completed here, not only because of their importance to university projects and past research, but also because the datasets' text content differs a great deal.

.pull-left[
**NHTSA**:

- Lengthy documents.

- Formal language. 

- Small corpus.
]
.pull-right[
**reddit**:

- Short documents.

- Informal language; "netspeak" and slang.

- Large corpus.
]
---

# Introduction to Methods
## Why Graphs?

Graphs are a great model for natural language processing!

--

- Graph representations of text can be composed using bigrams, n-grams, _skip-grams_, etc.

--

- Graph representations of text preserve more of the rich structures of language, e.g. phrases, idioms, and figures of speech.

--

- Avoid relying on assumptions about underlying distribution of words like some bag-of-words models do.

--

- Graph representations allow for a whole new set of methods to be applied to the data.

---

# Introduction to Methods (cont.)
## Example of Skip-gram Graph
.pull-up[
```{r, out.width = "400px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_network.png")
```

_Note:_ skip grams with window width $k=2$.
]
.pull-down[


Notice we have some phrases/ideas represented in the graph:

- "stuttering" [in a] "meeting" [with] "bosses"

- "people" "feel" "approachable"

- "feel" "aura" "exude(s)" "coldness"
]
---

# Introduction to Methods (cont.)
## Why Graph Kernels?

_Graph kernels_ are a family of methods which are used to compute similarity between graphs, which then can be used as a kernel in other applications. The kernel  matrix is then able to be used in methods like support vector machines, or in this case hierarchical clustering.

In this case a specific type of graph kernel is used: the edge histogram kernel. The edge histogram kernel has the advantage of a more efficient computation than other, more complicated, graph kernels. Since the reddit dataset has such a large corpus and the NHTSA dataset has such large documents, it was of particular concern that computation be efficient. 
---

# Introduction to Methods (cont.)
## How is the Kernel Matrix Used?

The graph kernel matrix is a _similarity matrix_ and the _hierarchical clustering_ that is used requires a distance matrix. For this a simple function is applied to take the element-wise multiplicative inverse, and we use the resulting matrix as a distance matrix in hierarchical clustering. 

Through the use of hierarchical clustering we obtain dendrograms that allow for clean, visual, interpretation of clustering results.

---

# Introduction to Methods (cont.)
## Example of Resulting Dendrogram

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_dendo.png")
```
---
class: inverse, center, middle
name: Methods

# Methods

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---
# Methods
## Workflow for Analysis

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/ScriptMap.png")
```

---
# Methods
## Data Ingest & Text Preprocessing

Text is brought in to the environment through one of the collection scripts. For reddit this means using the `{redditExtractor}` package, and for NHTSA this means collecting the data from the XML files. The text is then cleaned of punctuation, numeric values, stop words, and other words shorter than length $4$. This cleaning also involves _tokenization_ of the documents into _skip-grams_ (window width varying). 

This process uses popular packages from the R community such as `{dplyr}`, `{tidytext}`, and `{stringr}` (in addition to custom functions) to do data reformatting/reshaping, tokenization, and parsing of strings to properly treat the data before it moves to the next step.

The resulting data is then passed into data reformatting.

---
# Methods
## Data Reformatting

At this point, data needs to be transformed from a tabular, tidy format, to a graph representation. To do this each pair of words from the skip-gram tokenization forms an edge, and corresponding nodes, as a graph is constructed (`{igraph}` is used here). The graph object is then formatted in such a way that each word is a vertex, and each edge is a skip-gram that was present in the document. 

However, the graph is not ready to be processed yet, as the graph kernel functions deal with integer labels for the nodes. The labels for each vertex is then reassigned an integer value. Let $W$ be the word set of graph $G$, and $W'$ be the word set of graph $G'$. Then the mapping is: $f: (W \cup W') \rightarrow I$, where $I = \{i \in \mathbb{Z}| 0 < i < (W \cup W')\}$. 

Data from reddit is also split into smaller datasets to make use of GitHub's storage limits.

---

# Methods (cont.)
## Data Reformatting 

```{r, out.width = "500px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/Concept_map.png")
```

---

# Methods (cont.)
## Computation of the Kernel

The edge label histogram kernel is a sum of the number of label appearances; in the case of this application however, a label appears once, regardless of the number of appearances.
For this reason, later in analysis, manhattan distances are used instead of euclidean distances.

--

A edge label histogram is defined as: $\vec{g} = (g_1,g_2, ...,g_i)$
--
, such that $g_i = |\{(u,v) \in E|\phi(u,v) = i\}|$
--
, where $(u,v)$ is an edge, $E$ is the edge set in graph $G$, and $\phi$ is the function that maps edges to a scalar value in the range of unique values.

---

# Methods (cont.)
## Computation of the Kernel

Once there are two edge label histograms, $\vec{g}$ and $\vec{g}'$, they are compared with a kernel function $K$.

--

Either a linear kernel: 

$K(\vec{g}, \vec{g}') = \vec{g}^T \vec{g}'$

--

or a Gaussian Radial Basis Function (RBF) Kernel: 

$K(\vec{g}, \vec{g}') = e^{-\left( \frac{||\vec{g} - \vec{g}'||^2}{2\sigma^2}\right) }$

--

The result of either kernel, when applied to the document graph list, is a kernel matrix with dimensions $n \times n$ for a list of $n$ graphs.
---

# Methods (cont.)
## Hierarchical Clustering

Before using the kernel matrix in hierarchical clustering, two transformations are performed. 

--

First the multiplicative inverse is computed for each entry in the matrix, by doing this, the similarity matrix is converted to a distance matrix. 

--

Second, principal component analysis is performed to reduce the high dimensionality of the kernel matrix, and allow for better visualizations.

---

# Methods (cont.)
## Hierarchical Clustering

Hierarchical Clustering is performed on both data sets using the Manhattan distance and Ward's method for linkage in clustering. 

Manhattan Distance: $d(\vec{p},\vec{q}) = \sum_{i=1}^n |p_i - q_i|$

Ward's method: $d(\vec{p},\vec{q}) = || \vec{p} - \vec{q} ||^2$

--

As mentioned earlier, the edge label histograms are all either values of $1$ or $0$, since the skip-gram either appeared or did not. So the Manhattan distance is used, as opposed to euclidean distance.

---

# Methods (cont.)
## Parallel Processing

```{r, out.width = "450px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/edgeHistTimePlot.png")
```

---

# Methods (cont.)
## Parallel Processing

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/ScriptMap.png")
```

---

# Methods 
## Parallel Processing

Since the data reformatting and computing the kernels takes so long, it was necessary to implement parallel processing. This was done through the use of the `{furrr}` package for R. This parallel processing was necessary for the reddit dataset due to the large number of documents, but not for the NHTSA SCI reports despite the large document sizes.

Using parallel processing allowed for the reddit data to be computed in a reasonable time instead of unreasonable times like shown in the study previously.

---

class: inverse, center, middle
name: Analysis

# Analysis

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---


# Analysis
## NHTSA SCI
.pull-left[
First, hierarchical clustering was performed for a number of hyperparameter sets to assess which set produced the best clustering, measuring by within sum of squares (WSS).

The five best hyperparameters sets were collected and used in further analysis of NHTSA data.
]
.pull-right[
```{r, out.width = "475px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/hclust_variation.png")
```
]

---

# Analysis (cont.)
## NHTSA SCI

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/table_pic.png")
```

---

# Analysis (cont.)
## NHTSA SCI

```{r, out.width = "750px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/5cluster.png")
```


---

# Analysis: NHTSA SCI

In the 5 dendrograms produced, we see that prominent clusters form in the dendrograms. Furthermore, there are a number of documents that are co-members across all 5 clusterings; this is encouraging as it indicates that the those clusters are not so sensitive to chosen hyperparameters, but the documents are indeed similar.

We can track this co-membership across all 5 clusterings with a matrix where rows and columns are documents, and the entry is either a $1$ or $0$ depending on if the document pair were consistently co-members.


---

# Analysis: NHTSA SCI

```{r, out.width = "550px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/comembers5.png")
```

--- 

# Analysis: NHTSA SCI

Upon further inspection of some co-members text we can notices some similarities. For example, document #2 (Ambulance crash in Angola, DE) has only 3 other co-members; document #11, #30, and #38. Three featured ambulance roll over events, all had fatalities, and all of them were front end collisions where the ambulance struck another vehicle, structure, or object.

While this is just one specific example of similarities across the co-members, we can use text mining concepts like term-frequency/inverse document frequency (TF-IDF) plots to examine which words may represent a cluster.

---

# Analysis: NHTSA SCI

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/nhtsa_tf_idf.png")
```

---
# Analysis: NHTSA SCI

In the TF-IDF plot we see that clusters use words specifically related to a cluster's vehicle manufacturers, medical terms, and words that describe a crash situation (e.g. guardrail, tree, interstate).

These TF-IDF plots stand as a summary for each cluster, and to gain more insight into a cluster's content we can examine the skip-gram graph.

---

# Analysis: NHTSA SCI

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/graph_k5_2.png")
```

---

# Analysis: reddit Threads

Similar to the NHTSA SCI reports, optimal number of clusters was assessed with an "elbow plot" and the consistently best performing hyperparamter set from the NHTSA analysis was used here as well.

Hyperparameters: Skip window $k=3$, kernel parameter $\sigma = 1200$

After computing the kernel with the hyperparameter set, we see two very distinct larger groups which then break into smaller groups. The groups are clear and provide confidence in the clustering result. In addition to the dendrogram, analysis is done on the cluster to assess which posts were included in the cluster as it relates to their query word that lead to their collection.

---

# Analysis: reddit Threads

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_wss.png")
```

---

# Analysis: reddit Threads

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/k4_reddit_clusters.png")
```

---

# Analysis: reddit Threads

We see that some query words are more strongly expressed in a cluster than other words. This may be indicative of a writing style, discussion topic, or tone that is more unique to those threads. 

In the previous slide, we see that clusters $3$ & $4$ collected posts from the "angry" query word, and cluster $1$ displayed a strong portion of its posts being collected from the "sad" query word.

Again, we can view a skip-gram network to get an idea of what the text was discussing. Below we can visualize the skip-gram network for $10$ threads that were clustered together.

---

# Analysis: reddit Threads

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_network.png")
```

---

# Analysis: Comparison of Results

Upon examination of both results, it is clear that these methods perform better with larger documents, as opposed to a large corpus. By measure of WSS, the NHTSA dataset displayed better results than the reddit dataset. The reddit dataset did however display more prominent clusters, which could have been due to the large number of documents in that corpus. Additionally, the computation time for the large document size NHTSA dataset was much more reasonable than the large corpus reddit dataset.

--

It should also be noted that the optimal number of clusters, as well as hyperparameters, will need to be adjusted and changed depending on the dataset of concern. Applying results from the NHTSA study, which indicated that $k=3$ was the best skip-gram window width, did seem to apply to the reddit dataset as well.

---
class: inverse, center, middle
name: Conclusion

# Conclusion

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Conclusion

Through the application of several different methods in conjunction with one another (skip-grams, graph kernels, and hierarchical clustering), clustering can be performed on text while preserving as much of the rich information from the original text as possible.

--

The NHTSA results can be used to inform other research initiatives of similar scenarios to simulate or reproduce for autonomous vehicles to learn from. Identifying the clusters and providing summarized results, allows for similar crash reports to be compiled. This compiled information can aid in the generation of more "edge case" scenarios for Florida Polytechnic University's AMI. 

--

The reddit thread dataset, although too large for these methods to be applied initially, but through use of parallel processing it was able to be completed. The prominent clustering from this dataset was an encouraging result, and with some additions to these methods the workflow outlined here could be a strong social media mining tool. 

---

# Conclusion

In summary, the methods described here will continue to develop into more mature technologies that could be applied to a wide range of natural language processing tasks. Once these methods are rebuilt into a clean package for a high performance computing environment, they will see use in industries that process large amounts of text data. 

--

Additionally, these methods could be applied to previously published work on the intelligent navigation of comments and posts for social media. The clustering results here could prove to be another clever way to filter out harmful, or otherwise unwanted, content from a web user. 

--

All of the methods are publically available on a [public GitHub repository](https://github.com/Levi-Nicklas/GraphDocNLP). All code can be updated, adapted, recreated, and scrutinized; transparency and availability of these methods will benefit the software development and data science communities that use GitHub. The open sourcing of software leaves the work open, and leads to better open science for the community at large.

---

class: inverse, center, middle
name: Future

# Future Development

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---

The most important future work at this stage is adapting these methods to be able to be applied at scale on a high performance computing cluster (HPC). The methods currently do not handle a large corpus well, which will be problematic for any application in industry. 

--

Additionally, the methods need to be tested with additional modifications. Adding edge attributes which are weightings according to the number of times a skip-gram appears in the text, or a TF-IDF value, could be a modification which leads to better results. Other types of graph kernels, which are more computationally intensive, should be used on this same data and the results compared.

--

Lastly, since the NHTSA dataset is small, it could be labeled by hand and then supervised learning could be applied. Support vector machines could be a great first step in using the graph kernel matrix in a supervised learning context.

---


class: inverse, center, middle
name: End

# Any Questions?

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

class: inverse, center, middle
name: Close

# Thank you!!

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# References

--- 

Available at GitHub [here](https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/main.pdf#appendix*.19)
