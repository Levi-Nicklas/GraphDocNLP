---
title: "Graph Kernels as Preprocessing for Unsupervised Learning: Two Case Studies"
author: "Levi C. Nicklas (B.S. Applied Mathematics)"
subtitle: "<html><div style='float:left'></div><hr color='#522A87' size=1px width=796px></html>"
#author: Levi Nicklas
date: Masters of Science in Computer Science Candidate #"`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ["poly.css", metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(here)
knitr::opts_chunk$set(echo = FALSE)
```



class: inverse, center, middle
name: Intro

# Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Introduction
## The Big Idea

One of the many methods in text mining and text visualization is to express the text as a bi-gram, **skip-gram**, or other n-gram graphs, but can these ideas be extended into any machine learning applications?

--

Through use of **graph kernels** two, or more, graphs can be compared to one another. This resulting mesaure of similairty then can be used in a variety of machine leanring applications.

--

The application of interest here is to use the similarity measures to perform unsupervised learning in the form of **hierarchical clustering**.




---

# Introduction to the Data

Two data sets are use for all of the analyses, one coming from the National Highway Traffic Safety Administration ( [NHTSA](https://www.nhtsa.gov/research-data/special-crash-investigations-sci)) and one from the popular social media website [reddit.com](reddit.com/r/mentalhealth/). 

--

The NHTSA data are a series of special crash investigations (SCI) which are publically available through a portal in PDF format. The SCI program provides the agency with detailed results from investigative specialists who detail and carefully document the incident of concern. $(N=48)$

--

The reddit data are public online forum discussion on the `r\MentalHealth` subreddit which satisfied query criteria. The use of reddit in this particular topic is critical, as reddit is well known for the platform, and its users' strong commitement to anonymous online interactions. This type of environment often leads to candid and honest discussion about personal problems. $(N=1000)$
---

# Introduction to the Data (cont.)
## NHTSA SCI Reports

- **Motivation:** These efforts and results support iniatives in Florida Polytechnic University's Advanced Mobility Institute (AMI) which focuses in the development of connected and fully autonomous vehicles. These reports are all on incidents which involved ambulances in the collision. The events all represent "edge cases" that are rare and need to be characterized properly for AMI's future success in developing robust autonomous vehicles.

- **Format and Collection:** the reports are all stored as PDF files which are all converted to XML and from which the text of concern is then parsed out for further processing and analysis.

```{r, out.width = "400px", fig.retina = 3, echo  = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://crashviewer.nhtsa.dot.gov/Images/NHTSA_Logo_w_tag.png")
```
---

# Introduction to the Data (cont.)
## Mental Health Subreddit

- **Motivation:** Previously published research work was focused on text mining within reddit subreddits focused on opiate addiction. The paper focused on clustering of posts/comments from the data collected from the subreddit. Using this type of data with new methods builds upon my existing work.

- **Format and Collection:** the thread text is collected with `{redditExtractor}` and then must undergo a cleaning and reformating, as the tabular format is structured such that observational units are threads. 

```{r, out.width = "300px", fig.retina = 3, echo  = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://logos-download.com/wp-content/uploads/2016/06/Reddit_logo_full_1.png")
```

---

# Introduction to the Data (cont.)
## Comparison of Text

The datasets were chosen for the analyses completed here, not only because of their importance to university projects and past research, but also becuase the datasets' text content differs a great deal.

.pull-left[
**NHTSA**:

- Lengthy documents.

- Formal language. 

- Small corpus.
]
.pull-right[
**reddit**:

- Short documents.

- Informal language; "netspeak" and slang.

- Large corpus.
]
---

# Introduction to Methods
## Why Graphs?

Graphs are a great model for natural language processing!

--

- Graph representations of text can be composed using bigrams, n-grams, _skip-grams_, etc.

--

- Graph representations of text preserve more of the rich structures of language, e.g. phrases, idioms, and figures of speech.

--

- Avoid relying on assumptions about underlying distribution of words like some bag-of-words models do.

--

- Graph representations allow for a whole new set of methods to be applied to the data.

---

# Introduction to Methods (cont.)
## Example of Skip-gram Graph
.pull-up[
```{r, out.width = "400px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_network.png")
```

_Note:_ skip grams with window width $k=2$.
]
.pull-down[


Notice we have some phrases/ideas represented in the graph:

- "stuttering" [in a] "meeting" [with] "bosses"

- "people" "feel" "approachable"

- "feel" "aura" "exude(s)" "coldness"
]
---

# Introduction to Methods (cont.)
## Why Graph Kernels?

_Graph kernels_ are a family of methods which are used to compute similarity between graphs, which then can be used as a kernel in other applications. The kernel  matrix is then able to be used in methods like support vector machines, or in this case hierarchical clustering.

In this case a specific type of graph kernel is used: the edge histogram kernel. The edge histogram kernel has the advantage of a more efficient computation than other, more complicated, graph kernels. Since the reddit dataset has such a large corpus and the NHTSA dataset has such large documents, it was of particular concern that computation be efficient. 
---

# Introduction to Methods (cont.)
## How is the Kernel Matrix Used?

The graph kernel matrix is a _similarity matix_ and the _hierarchical clustering_ that is used requires a distance matrix. For this a simple function is applied to take the element-wise multiplicative inverse, and we use the resulting matrix as a distance matrix in hierarchical clustering. 

Through the use of hierarchical clustering we obtain dendrograms that allow for clean, visual, interpretation of clustering results.

---

# Introduction to Methods (cont.)
## Example of Resulting Dendrogram

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_dendo.png")
```
---
class: inverse, center, middle
name: Methods

# Methods

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---
# Methods
## Workflow for Analysis

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/ScriptMap.png")
```

---
# Methods
## Data Ingest & Text Preprocessing

Text is brought in to the environment through one of the collection scripts. For reddit this means using the `{redditExtractor}` package, and for NHTSA this means collecting the data from the XML files. The text is then cleaned of punctuation, numeric values, stop words, and other words shorter than length $4$. This cleaning also involves _tokenization_ of the documents into _skip-grams_ (window width varying). 

This process uses popular packages from the R community such as `{dplyr}`, `{tidytext}`, and `{stringr}` (in addtion to custom functions) to do data reformatting/reshaping, tokenization, and parsing of strings to properly treat the data before it moves to the next step.

The resulting data is then passed into data reformatting.

---
# Methods
## Data Reformatting

At this point, data needs to be transformed from a tabular, tidy format, to a graph representation. To do this each pair of words from the skip-gram tokenization forms an edge, and corresponding nodes, as a graph is constructed (`{igraph}` is used here). The graph object is then formatted in such a way that each word is a vertex, and each edge is a skip-gram that was present in the document. 

However, the graph is not ready to be processed yet, as the graph kernel functions deal with integer labels for the nodes. The labels for each vertex is then reassigned an integer value. Let $W$ be the word set of graph $G$, and $W'$ be the word set of graph $G'$. Then the mapping is: $f: (W \cup W') \rightarrow X$, where $X = \{x \in X| 0 < x < (W \cup W')\}$. 

Data from reddit is also split into smaller datasets to make use of GitHub's storage limits.

---

# Methods (cont.)
## Data Reformatting 

```{r, out.width = "500px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/Concept_map.png")
```

---

# Methods (cont.)
## Computation of the Kernel

The edge label histogram kernel is a sum of the number of label appearances; in the case of this application however, a label appears once, regardless of the number of appearances.
For this reason, later in analysis, manhattan distances are used instead of euclidean distances.

--

A edge label histogram is defined as: $\vec{g} = (g_1,g_2, ...,g_i)$
--
, such that $g_i = |\{(u,v) \in E|\phi(u,v) = i\}|$
--
, where $(u,v)$ is an edge, $E$ is the edge set in graph $G$, and $\phi$ is the function that maps edges to a scalar value in the range of unique values.

---

# Methods (cont.)
## Computation of the Kernel

Once there are two edge label histograms, $\vec{g}$ and $\vec{g}'$, they are compared with a kernel function $K$.

--

Either a linear kernel: 

$K(\vec{g}, \vec{g}') = \vec{g}^T \vec{g}'$

--

or a Guassian Radial Basis Function (RBF) Kernel: 

$K(\vec{g}, \vec{g}') = e^{-\left( \frac{||\vec{g} - \vec{g}'||^2}{2\sigma^2}\right) }$

--

The result of either kernel, when applied to the document graph list, is a kernel matrix with dimensions $n \times n$ for a list of $n$ graphs.
---

# Methods (cont.)
## Hierarchical Clustering

Before using the kernel matrix in hierarchical clustering, two transformations are performed. 

--

First the multiplicative inverse is computed for each entry in the matrix, by doing this, the similarity matrix is converted to a distance matrix. 

--

Second, principal component analysis is performed to reduce the high dimensionality of the kernel matrix, and allow for better visualizations.

---

# Methods (cont.)
## Hierarchical Clustering

Hierarchical Clustering is performed on both data sets using the manhattan distance and Ward's method for linkage in clustering. 

Manhattan Distance: $d(\vec{p},\vec{q}) = \sum_{i=1}^n |p_i q_i|$

Ward's method: $d(\vec{p},\vec{q}) = || \vec{p} - \vec{q} ||^2$

--

As mentioned earlier, the edge label histograms are all either values of $1$ or $0$, since the skip-gram either appeared or did not. So the manhattan distance is used, as opposed to euclidean distance.

---

# Methods (cont.)
## Parallel Processing

```{r, out.width = "450px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/edgeHistTimePlot.png")
```

---

# Methods (cont.)
## Parallel Processing

```{r, out.width = "600px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/ScriptMap.png")
```

---

# Methods 
## Parallel Processing

Since the data reformatting and computing the kernels takes so long, it was necessary to implement parallel processing. This was done through the use of the `{furrr}` package for R. This parallel processing was necessary for the reddit datset due to the large number of documents, but not for the NHTSA SCI reports despite the large document sizes.

Using parallel processing allowed for the reddit data to be computed in a reasonable time instead of unreasonable times like shown in the study previously.

---

class: inverse, center, middle
name: Analysis

# Analysis

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---


# Analysis
## NHTSA SCI
.pull-left[
First, hierarchical clustering was performed for a number of hyperparameter sets to assess which set produced the best clustering, measuring by within sum of squares (WSS).

The five best hyperparameters sets were collected and used in further analysis of NHTSA data.
]
.pull-right[
```{r, out.width = "475px", fig.retina = 3, echo  = FALSE,fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/hclust_variation.png")
```
]

---

# Analysis (cont.)
## NHTSA SCI

```{r, out.width = "700px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/table_pic.png")
```

---

# Analysis (cont.)
## NHTSA SCI

```{r, out.width = "750px", fig.retina = 3, echo = FALSE, fig.align='center'}
# OPEN IN CHROME ONLY
knitr::include_graphics("https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/5cluster.png")
```



