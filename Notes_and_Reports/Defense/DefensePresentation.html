<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Graph Kernels as Preprocessing for Unsupervised Learning: Two Case Studies</title>
    <meta charset="utf-8" />
    <meta name="author" content="Levi C. Nicklas (B.S. Applied Mathematics)" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="poly.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Graph Kernels as Preprocessing for Unsupervised Learning: Two Case Studies
## <html>
<div style="float:left">

</div>
<hr color='#522A87' size=1px width=796px>
</html>
### Levi C. Nicklas (B.S. Applied Mathematics)
### Masters of Science in Computer Science Candidate

---






class: inverse, center, middle
name: Intro

# Introduction

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Introduction to the Data

Two data sets are use for all of the analyses, one coming from the National Highway Traffic Safety Administration (NHTSA) and one from the popular social media website [reddit.com](reddit.com/r/mentalhealth/). 

The NHTSA data are a series of special crash investigations (SCI) which are publically available through a portal in PDF format. `\((N=48)\)`

The reddit data are public online forum discussion on the `r\MentalHealth` subreddit which satisfied query criteria. `\((N=1000)\)`
---

# Introduction to the Data (cont.)
## NHTSA SCI Reports

- **Motivation:** These efforts and results support iniatives in Florida Polytechnic University's Advanced Mobility Institute (AMI) which focuses in the development of connected and fully autonomous vehicles. These reports are all on incidents which involved ambulances in the collision. The events all represent "edge cases" that are rare and need to be characterized properly for AMI's future success in developing robust autonomous vehicles.

- **Format and Collection:** the reports are all stored as PDF files which are all converted to XML and from which the text of concern is then parsed out for further processing and analysis.

&lt;img src="https://crashviewer.nhtsa.dot.gov/Images/NHTSA_Logo_w_tag.png" width="400px" /&gt;
---

# Introduction to the Data (cont.)
## Mental Health Subreddit

- **Motivation:** Previously published research work was focused on text mining within reddit subreddits focused on opiate addiction. The paper focused on clustering of posts/comments from the data collected from the subreddit. Using this type of data with new methods builds upon my existing work.

- **Format and Collection:** the thread text is collected with `{redditExtractor}` and then must undergo a cleaning and reformating, as the tabular format is structured such that observational units are threads. 

&lt;img src="https://logos-download.com/wp-content/uploads/2016/06/Reddit_logo_full_1.png" width="300px" /&gt;

---

# Introduction to the Data (cont.)
## Comparison of Text

The datasets were chosen for the analyses completed here, not only because of their importance to university projects and past research, but also becuase the datasets' text content differs a great deal.

.pull-left[
**NHTSA**:

- Lengthy documents.

- Formal language. 

- Small corpus.
]
.pull-right[
**reddit**:

- Short documents.

- Informal language; "netspeak" and slang.

- Large corpus.
]
---

# Introduction to Methods
## Why Graphs?

Graphs are a great model for natural language processing!

- Graph representations of text can be composed using bigrams, n-grams, _skip-grams_, etc.

- Graph representations of text preserve more of the rich structures of language, e.g. phrases, idioms, and figures of speech.

- Avoid relying on assumptions about underlying distribution of words like some bag-of-words models do.

- Graph representations allow for a whole new set of methods to be applied to the data.

---

# Introduction to Methods (cont.)
## Example of Skip-gram Graph
.pull-up[
&lt;img src="https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_network.png" width="400px" /&gt;

_Note:_ skip grams with window width `\(k=2\)`.
]
.pull-down[


Notice we have some phrases/ideas represented in the graph:

- "stuttering" [in a] "meeting" [with] "bosses"

- "people" "feel" "approachable"

- "feel" "aura" "exude(s)" "coldness"
]
---

# Introduction to Methods (cont.)
## Why Graph Kernels?

_Graph kernels_ are a family of methods which are used to compute similarity between graphs, which then can be used as a kernel in other applications. The kernel  matrix is then able to be used in methods like support vector machines, or in this case hierarchical clustering.

In this case a specific type of graph kernel is used: the edge histogram kernel. The edge histogram kernel has the advantage of a more efficient computation than other, more complicated, graph kernels. Since the reddit dataset has such a large corpus and the NHTSA dataset has such large documents, it was of particular concern that computation be efficient. 
---

# Introduction to Methods (cont.)
## How is the Kernel Matrix Used?

The graph kernel matrix is a _similarity matix_ and the _hierarchical clustering_ that is used requires a distance matrix. For this a simple function is applied to take the element-wise multiplicative inverse, and we use the resulting matrix as a distance matrix in hierarchical clustering. 

Through the use of hierarchical clustering we obtain dendrograms that allow for clean, visual, interpretation of clustering results.

---

# Introduction to Methods (cont.)
## Example of Resulting Dendrogram

&lt;img src="https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/reddit_dendo.png" width="600px" /&gt;
---
class: inverse, center, middle
name: Methods

# Methods

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;
---
# Methods
## Workflow for Analysis

&lt;img src="https://raw.githubusercontent.com/Levi-Nicklas/GraphDocNLP/main/Thesis_Tex/Content/Images/ScriptMap.png" width="600px" /&gt;

---
# Methods
## Data Ingest &amp; Text Preprocessing

Text is brought in to the environment through one of the collection scripts. For reddit this means using the `{redditExtractor}` package, and for NHTSA this means collecting the data from the XML files. The text is then cleaned of punctuation, numeric values, stop words, and other words shorter than length `\(4\)`. This cleaning also involves _tokenization_ of the documents into _skip-grams_ (window width varying). 

This process uses popular packages from the R community such as `{dplyr}`, `{tidytext}`, and `{stringr}` (in addtion to custom functions) to do data reformatting/reshaping, tokenization, and parsing of strings to properly treat the data before it moves to the next step.

The resulting data is then passed into data reformatting.

---
# Methods
## Data Reformatting

At this point, data needs to be transformed from a tabular, tidy format, to a graph representation. To do this each pair of words from the skip-gram tokenization forms an edge, and corresponding nodes, as a graph is constructed (`{igraph}` is used here). The graph object is then formatted in such a way that each word is a vertex, and each edge is a skip-gram that was present in the document. 

However, the graph is not ready to be processed yet, as the graph kernel functions deal with integer labels for the nodes. The labels for each vertex is then reassigned an integer value such that the word set `\(W\)` is mapped to integer values. This is done as: `\(f: W \rightarrow X\)` such that `\(X = \{x \in X | 0 &lt; x &lt; |W|\}\)`. (EDIT THIS TO BE THE MAGNITUDE OF THE UNION OF TWO GRAPHS)

Data from reddit is also split into smaller datasets to make use of GitHub's storage limits.

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": false,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
