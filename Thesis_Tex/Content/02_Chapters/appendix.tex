%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Thesis template by Youssif Al-Nashif
%
%   May 2020
%
%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
%\appendixpage
%\addappheadtotoc
\titleformat{\chapter}
{\normalfont\LARGE\scshape}{Appendix \thechapter: }{.1em}{#1\vspace{1ex}}[\titlerule]


% appendixes go here
\begin{appendices}
\chapter{Major Code Modules Used}


\section{data\_collect\_mentalhealth.R}
\begin{lstlisting}
### Reddit Data Collect
### Levi C. Nicklas
### 10/25/20
###
###
### Notes:  This data collected will be from reddit. 
###         Target data is that which related to mental
###         health and the open forum discussion of 
###         the issues, stigmas, and other challenges
###         surrounding those experiencing mental health
###         issues.
###
###         The package {RedditExtractoR} will be used 
###         to obtain the data. This will then be written
###         out to the data folder in an .RDS or .csv format.

library(tidyverse)
library(RedditExtractoR)


mental_health <- c("anxious", "anxiety", 
                   "depressed", "depression",
                   "mental", "illness", "scared",
                   "afraid", "sad", "emotion", "anger",
                   "angry", "upset", "suicide", "abuse",
                   "emotional","help", "addiction")

reddit_data <- list()

for(i in 1:length(mental_health)){
  reddit_temp <- RedditExtractoR::get_reddit(search_terms = mental_health[i], 
                             #regex_filter = ,
                             subreddit = "mentalhealth",
                             cn_threshold = 10,
                             sort_by = "comments",
                             wait_time = 3)
  
  reddit_data[[i]] <- reddit_temp
}

write_rds(reddit_data, "Data/RawData/mentalhealth_reddit_20201025.RDS")

}

rm(reddit_thread_kernel)
\end{lstlisting}

\section{correcting\_vertex\_labels.R}
\begin{lstlisting}
# convert_vertex_labels
# Levi C. Nicklas
# 1/20/2021 -- *Happy Inauguration Day*
#
#
# Notes:  This function converts text labels of two graphs and maps them to integer values.
#         {graphkernels} only works with integer labels on verticies. The function finds 
#         the union of unique words/labels and then maps each unique label to an integer
#         and then assigns the corresponding label to the correct vertex.

convert_vertex_labels <-  function(g1, g2){
  # Libs Req.
  require(magrittr)
  require(dplyr)
  require(igraph)
  require(graphkernels)
  
  # Grab vertex labels (words).
  v.g1 <- unlist(get.vertex.attribute(g1))
  v.g2 <- unlist(get.vertex.attribute(g2))
  
  # Build a dataframe that maps words to integers (1 to 1).
  map_to_int <- seq(1,length(unique(c(v.g1,v.g2))))
  map_to_int <- data.frame(word = unique(unique(c(v.g1,v.g2))),
                           int = map_to_int)
  
  # Change g1 vertex names.
  vertex.attributes(g1)$name <- left_join(data.frame(word = vertex.attributes(g1)$name), map_to_int, by = "word") %>% 
    select(int) %>% pull()
  
  # Change g2 vertex names.
  vertex.attributes(g2)$name <- left_join(data.frame(word = vertex.attributes(g2)$name), map_to_int, by = "word") %>% 
    select(int) %>% pull()
  
  return(list(g1,g2))
}


### Worked Example -- not functionized ###
# g1 <- reddit_graphs_s[[1]][[1]]
# g2 <- reddit_graphs_s[[2]][[1]]
# 
# v.g1 <- unlist(get.vertex.attribute(g1))
# v.g2 <- unlist(get.vertex.attribute(g2))
# 
# unique(c(v.g1,v.g2))
# 
# map_to_int <- seq(1,length(unique(c(v.g1,v.g2))))
# 
# map_to_int <- data.frame(word = unique(unique(c(v.g1,v.g2))),
#                          int = map_to_int)
# 
# vertex.attributes(g1)$name <- left_join(data.frame(word = vertex.attributes(g1)$name), map_to_int, by = "word") %>% 
#   select(int) %>% pull()
# 
# vertex.attributes(g2)$name <- left_join(data.frame(word = vertex.attributes(g2)$name), map_to_int, by = "word") %>% 
#   select(int) %>% pull()
# 
# CalculateVertexHistKernel(list(g1,g2))
# CalculateVertexHistKernel(list(reddit_graphs_s[[1]][[1]],
#                                reddit_graphs_s[[2]][[1]]))
\end{lstlisting}

\section{convert\_to\_graphs.R}
\begin{lstlisting}
### Reddit df -> skip_gram graph
### Levi C. Nicklas
### 12/30/20
###
###
### Notes:  


convert_to_graphs <- function(){
  # Libraries
  library(here)
  library(furrr)
  
  plan(multisession, workers = 4)
  
  ### REDDIT POSTS ###
  
  post_thread_graphs <- list()

  source(here::here("Development/Scripts/df_to_graph_list.R"))
  
  text_graphs <- furrr::future_map(.x = post_thread_text_sample$text,
                                   .f = df_to_graph_list)
  return(text_graphs)
  
  ### NHTSA PAPERS ###
  # papers <- readRDS(here::here("Data/NHTSA/RawData/ConsolidatedPapers.rds"))
  # 
  # # import function
  # source(here::here("Development/Scripts/df_to_graph_list.R"))
  # text_graphs <- furrr::future_map(.x = papers$text,
  #                                 .f = df_to_graph_list)
  # 
  # saveRDS(text_graphs, "Data/NHTSA/ProcessedData/nhtsa_graphs.RDS")
}
\end{lstlisting}

\section{compute\_kernel.R}
\begin{lstlisting}
# Compute Graph Kernels
# Levi C. Nicklas
# Date: 1/4/2021
#
#
# Notes:  
#


compute_kernel <- function(){
  # Libraries
  require(furrr)
  
  plan(multisession, workers = 8)
  
  text_kernel <- furrr::future_map(.x = thread_graphs,
                                   .f = compute_graph_similarity,
                                   .options = furrr_options(seed = T))
  
  #saveRDS(text_kernel, "Data/ProcessedData/reddit_graphkernel_325.RDS")
  return(text_kernel)
  saveRDS(text_kernel, "Data/ProcessedData/redditthreads_graphkernel_all.RDS")
}
\end{lstlisting}

\section{df\_to\_graph\_list.R}
\begin{lstlisting}
# Process into Graph Objects
# Levi C. Nicklas
# Date: 12/19/2020
#
#
# Notes:  This script features a function which takes a text vector
#         and returns a list of graph representations of the text file.

df_to_graph_list <- function(text){
  # Check libraries
  require(dplyr)
  require(tidyr)
  require(tidytext)
  require(igraph)
  
  n_gram <- 2
  k_skip <- 2
  
  # Prepare variables and space.
  text_df <- as.data.frame(text)
  colnames(text_df) <- c("text")
  text_df$id <- seq(1,nrow(text_df))
  graph_list <- list()
  
  # Loop over rows in data.
  for(i in 1:nrow(text_df)){
    # tokenize as skip grams.
    temp_graph <- tidytext::unnest_tokens(text_df, 
                                       output = "words" ,
                                       input = text, 
                                       token = "skip_ngrams",
                                       n = n_gram,
                                       k = k_skip) %>%  
      # filter to only 1 document.
      dplyr::filter(id == i) %>% 
      # split bigram
      tidyr::separate(words, c("word1", "word2"), sep = " ") %>%
      # remove stop words
      anti_join(stop_words, by = c("word1" = "word")) %>% 
      anti_join(stop_words, by = c("word2" = "word")) %>% 
      # toss NA values.
      tidyr::drop_na() %>% 
      # clean out punctuation %>% 
      mutate(word1_toss = stringr::str_detect(word1, "\\.")) %>% 
      mutate(word2_toss = stringr::str_detect(word2, "\\.")) %>% 
      mutate(toss_pair = word1_toss|word2_toss) %>% 
      filter(toss_pair == F) %>% 
      # clean out numebrs
      select(id,word1,word2) %>% 
      mutate(word1_toss = stringr::str_detect(word1,"[:digit:]")) %>% 
      mutate(word2_toss = stringr::str_detect(word2,"[:digit:]")) %>% 
      mutate(toss_pair = word1_toss|word2_toss) %>% 
      filter(toss_pair == F) %>% 
      select(id,word1,word2) %>% 
      # only keep words of > len 3
      mutate(word1_toss = (nchar(word1)<4)) %>% 
      mutate(word2_toss = (nchar(word2)<4)) %>% 
      mutate(toss_pair = word1_toss|word2_toss) %>% 
      filter(toss_pair == F) %>% 
      select(id, word1, word2) %>% 
      # group each bigram.
      group_by(word1, word2) %>% 
      # count occurances.
      count() %>% 
      # produce Graph.
      igraph::graph_from_data_frame()
    
    # cleaned_words_df <- temp_graph %>% igraph::clusters()
    # cleaned_words_df <- as.data.frame(cleaned_words_df$membership)
    # cleaned_words_df$words <- rownames(cleaned_words_df)
    # colnames(cleaned_words_df) <- c("member", "words")
    # reduced_clusters <- cleaned_words_df %>% 
    #   filter(member != 1)
    # 
    # # Get single largest cluster.
    # big_graph <- temp_graph %>% 
    #   as_edgelist() %>% 
    #   as.data.frame() %>% 
    #   anti_join(y = reduced_clusters, by = c("V1" = "words")) %>% 
    #   anti_join(y = reduced_clusters, by = c("V2" = "words")) %>% 
    #   graph_from_data_frame()
    
    # Store
    #graph_list[[i]] <- list(big_graph, text)
    graph_list[[i]] <- list(temp_graph, text)
    
    # if(i %% 1000 == 0){
    #   print(paste0("On Row: ",i))
    # }
  }
  
  

  # Return a list of Graphs  
  return(graph_list)
  
}
\end{lstlisting}

\section{compute\_graph\_similarity.R}
\begin{lstlisting}
# Compute Graph Similarity
# Levi C. Nicklas
# Date: 1/4/2021
#
#
# Notes:  
#


compute_graph_similarity <- function(input_graph){
  require(graphkernels)
  require(igraph)
  require(here)
  
  source(here::here("Development/Scripts/convert_vertex_labels.R"))
  
  # Edit Graph List to Op on.
  #graph_list <- reddit_graphs_s
  #graph_list <- nhtsa_graphs
  graph_list <- thread_graphs
  
  #Allocate Storage
  result <- rep(0,length(graph_list))
  
  #Compute Similarity btwn 1 graph and all others. FOR REDDIT
  for(i in 1:length(graph_list)){
    if(length(igraph::vertex.attributes(graph_list[[i]][[1]][[1]])) > 0 &
       length(igraph::vertex.attributes(input_graph[[1]][[1]])) > 0 ){

      # Correct Labels Issue.
      tmp_graph_list <- convert_vertex_labels(graph_list[[i]][[1]][[1]], input_graph[[1]][[1]])
      #print(paste0("Calculating Graph: #",i))
      #K <- graphkernels::CalculateEdgeHistKernel(tmp_graph_list)
      K <- graphkernels::CalculateEdgeHistGaussKernel(tmp_graph_list,1200)
      similarity_value <- K[1,2]
      result[i] <- similarity_value
    } else {
      #print(paste0("Skipping Graph: #",i))
      result[i] <- NA
    }
  }
  
  # FOR NHTSA
  # for(i in 1:length(graph_list)){
  #   if(length(igraph::vertex.attributes(graph_list[[i]][[1]])) > 0 & 
  #      length(igraph::vertex.attributes(input_graph[[1]])) > 0 ){
  #     
  #     # Correct Labels Issue.
  #     tmp_graph_list <- convert_vertex_labels(graph_list[[i]][[1]], input_graph[[1]])
  #     #print(paste0("Calculating Graph: #",i))
  #     #K <- graphkernels::CalculateEdgeHistKernel(tmp_graph_list)
  #     K <- graphkernels::CalculateEdgeHistGaussKernel(tmp_graph_list,1200)
  #     similarity_value <- K[1,2]
  #     result[i] <- similarity_value
  #   } else {
  #     #print(paste0("Skipping Graph: #",i))
  #     result[i] <- NA
  #   }
  # }
  
  return(result)
}
\end{lstlisting}


\chapter{NHTSA Metadata}
\begin{table}[]
\tiny
\begin{tabular}{lllll}
\multicolumn{1}{c}{\textbf{CASE \#}} & \multicolumn{1}{c}{\textbf{VEHICLE MAKE   /MODEL}} & \multicolumn{1}{c}{\textbf{LOCATION}} & \multicolumn{1}{c}{\textbf{FATALITIES\_DUE\_TO\_CRASH}} & \multicolumn{1}{c}{\textbf{ROLLOVER}} \\
CA08034 & 05 FORD E-450 & ANGOLA, DE & Yes & No \\
CA09005 & 98 FORD E-350 & LEXINGTON CO., SC & Yes & No \\
CA09040 & 09 FORD F-450 & BENNINGTON, VT & Yes & No \\
CA09075 & 04 FORD F-450 & FORT BRAGG, NC & Yes & No \\
CA09076 & 98 FORD E-350 & NASHVILLE, TN & Yes & No \\
CA09080 & 00 FORD E-350 & JOHNSON CO., TN & Yes & No \\
CA09082 & 04 FREIGHTLINER 2500 & MEMPHIS, TN & Yes & No \\
CA11004 & 07 GMC 3500 EXPRESS & BRIGHTON, NY & Yes & No \\
CA11026 & 01 FORD F-350 & CLARKSBURG, WV & Yes & Yes \\
CA11027 & 09 CHEVROLET G4500 & ORANGEBURG, SC & Yes & Yes \\
CA12002 & 12 FORD E-350 & BUCKINGHAM, VA & Yes & Yes \\
CA12030 & 01 FORD E-350 & MAPLETON DEPOT, PA & Yes & Yes \\
CA12032 & 09 CHEVROLET 4500 & SARPY, NE & No & Yes \\
CA12034 & 05 FORD ECONOLINE & GLENCOE, KY & Yes & Yes \\
CR12001 & 07   FORD E-450 & ALTOONA,   NY & Yes & No \\
CR12002 & 11   FORD E-350 & NEWARK,   NJ & Yes & Yes \\
CR13004 & 99   FORD E-350 & LEWIS   CO., WV & Yes & No \\
CR13021 & 2012   Chevrolet G4500 & OCILLA,   GA & Yes & No \\
CR14002 & 2012   FORD E-350 & MILFORD,   MA & Yes & Yes \\
CR14003 & 10   CHEVROLET G4500 & WINDSOR,   ME & Yes & Yes \\
CR14057 & 11 FORD F-350   AMBULANCE & MANSFIELD, OH & Yes & Yes \\
CR14063 & 12 FORD E-350 TYPE II & MARION CO., WV & Yes & Yes \\
CR15019 & 08 FORD E-SERIES & EAST BRUNSWICK, NJ & Yes & Yes \\
CR17012 & 2013 Chevrolet   Express 3500 Type II Ambulance & DUANESBURG, NY & Yes & No \\
CR18003 & 16 FORD F-450 TYPE I & STAFFORD CO., VA & Yes & Yes \\
IN10018 & 99 FORD E-350 & ST. LOUIS COUNTY, MN & Yes & Yes \\
IN10032 & 03 FORD E-350 & MCDOWELL CO., WV & Yes & Yes \\
IN10036 & 03 FORD E-350 & CLARK CO., IL & Yes & Yes \\
IN11001 & 09 FORD E-350 & CLARK CO., IN & Yes & No \\
IN11015 & 10 CHEVROLET G4500   EXPRESS & SHERIDAN, IN & Yes & Yes \\
IN11023 & 10 CHEVROLET G3500   EXPRESS & MILWAUKEE, WI & No & Yes \\
IN11024 & 96 FORD E-350 & FAIR PLAY, MO & Yes & No \\
IN13021 & 90 FORD E-350 & ELMWOOD, NE & Yes & No \\
IN13025 & 09 FORD E-450 & WAUPUN, WI & Yes & Yes \\
IN14004 & 12 FORD E-450 & BUFFALO, MN & Yes & Yes \\
IN14012 & 07 CHEVROLET EXPRESS   3500 & CARLSBAD, TX & Yes & Yes \\
IN14035 & 10 FORD ECONOLINE   E-350 & SAN ANTONIO, TX & Yes & No \\
IN16013 & 03 FORD ECONOLINE   E-350 & MARSHALL, MO & Yes & No \\
IN16025 & 11 FORD ECONOLINE   E-350 & CLYDE, OH & Yes & Yes \\
DS09001 & 05 FORD E-350 & PALO ALTO, CA & Yes & Yes \\
DS09019 & 99 FORD E-350 & TUCSON, AZ & Yes & Yes \\
DS11018 & 07 FORD E-350 & ONTARIO, CA & Yes & No \\
DS13021 & 00 FORD F-350 & CAMBRIDGE, ID & No & No \\
DS14016 & 13 MERCEDES-BENZ   SPRINTER & SIGNAL HILL, CA & No & Yes \\
DS15009 & 09 FORD E-350 & PASADENA, CA & Yes & No \\
DS15014 & 07 GMC YUKON & SPRINGFIELD, CO & Yes & Yes \\
DS16010 & 04 FORD E-350 TYPE II & MORGAN CO., CO & Yes & Yes \\
DS16014 & 09 FORD E-350 TYPE II & ALBUQUERQUE, NM & No & Yes
\end{tabular}
\end{table}

\chapter{KDE Clustering}
As an alternative to popular clustering methods that were used in the studies in this paper, another clustering method was attempted that features use of kernel density estimation. By using a kernel density estimation (KDE) on the kernel values, we can cluster the documents into similar groups, defined by local maxima. \\
First, the graph kernel matrix, $K$ is taken, and we extract a row, $i$, and we compute a KDE using R's \texttt{density()} function. Now, the default value for bandwidth will likely produce a smooth, unimodal or bimodal distribution, but this is not what the goal is. The goal is to use the KDE to find clusters through their value appearing in a local maxima. So, through producing a KDE with few local maxima, we produce very few clusters. If the number of clusters needs to increase, we can essentially overfit the KDE and manipulation the use of the bandwidth parameter to create a KDE with many more local maxima and minima.\\
Once a KDE with a sufficient number of local maxima, which is determined by the user, then cluster breaks are located. If we consider the estimated KDE to be a function $k(x)$, where $x$ is a kernel value, and $k(x)$ is the estimated density at a value $x$, then we can use calculus to locate the break points. \\
This method need additional development that was beyond the scope of this paper, and will be revisited at a later time. Code developed is available in the GitHub (\texttt{https://github.com/Levi-Nicklas/GraphDocNLP}).




\end{appendices}

