%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Thesis template by Youssif Al-Nashif
%
%   May 2020
%
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Processing Text into Graphs}

\hspace*{0.3cm} For both datasets, the reddit comments and the SCI papers, the text data is collected and stored in its raw form for reproducibility. The text is then cleaned and prepared for analysis using a suite of text processing tools from the \texttt{\{tidytext\}} package for R \cite{silge2016tidytext}. Using this package, the text undergoes tokenization into skip-grams, stop word removal, and filtering to remove punctuation, numbers, short words, etc. \\

Skip-grams are produced for $k=2,3,4$, where $k$ is the window width of the skip-gram for which bigrams are formed. Stop words are gathered from popular lexicons: ``snowball", ``SMART", and ``onix". As stated above, all numeric values, and lingering punctuation were also removed through use of the \texttt{\{stringR\}} package \cite{wickham2010stringr}. This processing is an essential preprocessing step for using graph kernels to gauge similarity of documents, because words and strings (like punctuation or numbers) that get repeated frequently across many documents in the set do not actually mean the documents are similar\textemdash they just have common reoccurring words. Removing these types of words forces the text data to be more unique across the document set.  \\

The result of the skip-gram tokenizer is a data frame where each observation is a bigram, a pair of words, that the skip-gram window captured. The data frame is then cleaned of stop words through using anti-joins on each bi-gram; any row with the appearance of a stop word in either position, the first or second word, was removed. The same method was applied for punctuation and numbers. The result was a data frame of bigrams which appeared in a fixed window width, $k$, of one another and where both words are of a length greater than 3 letters, not a stop word, and do not contain numbers or punctuation. This cleaned dataset is now ready to be converted into a graph object. \\

Now that the skip-grams are cleaned, the data frame can be converted to a graph object, through use of the \texttt{\{igraph\}} package \cite{csardi2013package}. To do this, each word pair in the data frame is converted to an edge and vertex pair in the graph object. For example, the word pair ``data frame" will become two vertices, labeled ``data" and ``frame", with an undirected edge connecting them. In this study, directed graph edges are not used, but could be considered in future iterations of this work. When this completed for all the skip-gram pairs that were generated, it produces a singular connected graph, however a great deal of filtering occurred and there may be disconnected portions. When words are removed from the graph, a vertex will disappear and can potentially split the graph. This is not too common in the NHTSA dataset for two reasons. First, the text data is quite long, and so if a word is reused at a later time in the text there will be an additional connection to keep it included in the main graph. Secondly, the benefit of the skip-gram, as opposed to plain bigrams, is that the larger window width means words get connected and can ``skip" over words that will get removed through the data cleaning process. So at this point, if there is a group of words that are isolated and not connected to the main graph, they are often quite small and are only several words. To keep computation simple, these lingering small isolated graphs are removed. The vertices that are not members of the main graph are removed from the graph object and that leaves a single connected graph for each text document. These graphs can be compared with graph kernels easily now. 


