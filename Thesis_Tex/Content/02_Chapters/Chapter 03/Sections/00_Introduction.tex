%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Thesis template by Youssif Al-Nashif
%
%   May 2020
%
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}


\subsection{ Skip-grams}
\hspace*{0.5cm} As an alternative to natural language processing (NLP) methods, which are reliant on "bag-of-words" methods, the methods used here utilize a graph representation of the text. Consider a bigram, a pair of two words\textemdash like "hot dog" or "peanut butter", these bigrams can be constructed for a text document where every pair or adjacent words is a bigram. The bigrams can then be used to make a graph, where each word is a vertex, and each bigram is an edge. This graph representation holds more context than the bag-of-words methods; for example seeing the words "cake" and "carrot" in a bag of words may not show that "carrot cake" was the real intent of the text. This is an important concept for modeling text, as we should strive to achieve a representation of the text that makes for effective modeling that will capture the true meaning of the text in question. Keeping this in mind, with the example of "carrot cake", what about the idiom "beating a dead horse"? Each word individually may mean something other than the idiom. Even the bigrams "beating dead" and "dead horse" do not capture what the idiom means. We can expand the number of words in the n-gram to be 3 or 4 words, or alternatively, we can make more "edges" or connect more words. We can connect words that are not immediately adjacent but perhaps within $k$ words away. These bigrams that appear within $k$ words of each other are called "skip-grams". The skip-gram allows to capture context of larger sequences of words since the graph representation will show how the $k$ wide neighborhood of words was connected. In the idiom example, using skip-grams with window width $k = 2$, and removing common words (e.g. "a", "at", "the"), will produce a graph like: 

$$
E(G) = \{
\text{beat}  \longleftrightarrow \text{dead}, 
\text{dead}  \longleftrightarrow \text{horse}, 
\text{horse}  \longleftrightarrow \text{beat} \}
$$

This graph representation contains a cycle, of length 3, where most native english speakers will identify the meaning behind the graph representation. As ideas, idioms, figures of speech, and other concepts (that may be explained in a non-literal fashion) grow in size as they include more words, it becomes more difficult to capture the meaning behind the text. However, leveraging the concept of a skip gram can produce such a rich graph representation of the text that the original meaning is more likely to be preserved. \\

\subsection{ Graph Kernels}
The next natural question is, "how can we compare these graph representations?", and we address this with graph kernel methods. These methods are used to compare the similarity of graphs. These use of a graph kernel to compare graphs was first published in 2003, and since various applications and adaptations have been made to the methods. In the case of text mining, the graph kernel must assess vertex labels \textemdash if one intends to map words to vertices, otherwise they will be assessing the topology alone. In this study, the Edge-Histogram kernel is the kernel used to compute similarity. This kernel was chosen as it uses labels on the graph structure, and is not as computationally intensive as other methods (CITE). In the specific implementation used for these studies, the computation time was the shortest when compared with other kernel methods like: graphlet, random walk, and Weisfeiler-Lehman kernel. (CITE) Since the data sets of concern in the studies feature either large graphs or a large number of graphs, the kernel had to be cheap computationally.\\

To compute and edge histogram kernel on two graphs, $G_1$ and $G_2$, first define the set of edges $E_i =\{ (u_1,v_1), (u_2,v_2), ... , (u_n,v_n) \}$ where $(u_n,v_n)$ is the $n$-th edge connecting $u_n$ to $v_n$. Then the edge label histogram is defined to be $\vec{g} = \{ g_1, g_2, ... , g_s\}$ where 
(FINISH THIS, link in comment)
%https://papers.nips.cc/paper/2015/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf

\subsection{Using Kernel for Clustering}

The output of the kernel is useful for a variety of tasks. Some other popular applications have included classification with support vector machines, which are popular with other kernel methods. In this case, the kernel is used for unsupervised clustering. Within the kernel matrix,$K$, the entry $k_{i,j}$ represents the similarity between graphs $i$ and $j$. This matrix which contains measures of similarity between points can be used as a distance matrix for hierarchical clustering. 





%--- Any subsection will go here.
