%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Thesis template by Youssif Al-Nashif
%
%   May 2020
%
%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Literature Review}
\label{literature-review}


%---- Introduction


\hspace*{0.3cm} The introduction of graph kernels first arose in 2002, from the paper ``\textit{Diffusion kernels on graphs and other discrete structures}" ~\cite{kondor2002diffusion}. Since then, graph kernels have been used in a variety of fields, and modified to suit different problems. These developments have resulted in applications in biology, chemistry, social media analytics, and machine learning. Graph kernels have since formed into 3 large sub-groups: random walk kernels, sub-graph (also called graphlet) kernels, and tree-based methods. Within these three categories there are a multitude of modifications which include things such as edge or vertex attributes into the calculations ~\cite{vishwanathan2010graph}. The graph kernels of particular interest to this study are those which incorporate vertex attributes into the calculation of the kernel.  This inclusion of vertex attributes will allow the assignment of a word from a text document to a vertex. \\

The use of graph kernels in text processing has been largely focused on applying them to a neural networks for NLP. However, a 2017 paper by Nikolentzos et.al, covered how their use first arrived at the idea to apply graph kernels to what they called ``graph-of-words" ~\cite{nikolentzos2017shortest}. They utilized a window width to construct a connections between words, effectively ``skipping" some words in between; this concept has also been called a ``skip-gram" network in other contexts ~\cite{cheng2006n}. Members of this research group, and others close to them, have produced software packages to perform some basic graph kernel methods. In addition, there have been a small number of papers from the group about the application of these methods to text mining ~\cite{sugiyama2018graphkernels}. Their work centered around use of neural networks for classification tasks. 
%%% TABLE %%%
\begin{table}[H]
\caption{Summary of Literature Review}
\centering
\begin{tabular}{ c c c c}
\hline
\hline
Topic & Author & Title & Year \\ [0.5ex]
\hline
Graph Kernels & Vishwanathan & Graph Kernels & 2010\\
Graph Kernels & Kriege et al. & A survey on graph kernels & 2020\\
Graph Kernels & Nikolentzos et al. & [graph kernels for doc. similarity] & 2017\\
Graph Kernels & Kondor and Lafferty &  Diffusion Kernels on Graphs [...] & 2002\\
Skip-Grams & Cheng et al. & From n-gram to skipgram [...] & 2006\\
Text Mining & Vazirgiannis et al. & GraphRep: [...] & 2018 \\
Application & Rosenfeld et al. & Kernel of Truth: [...] & 2020\\
Software & Silge and Robinson& tidytext &  2016\\
Software & Sugiyama et al. & graphkernels & 2018 \\
Software & Casardi et al. & igraph & 2013\\
\hline
\end{tabular}
\end{table}

The literature review provided some valuable insights which drove some critical decisions for the methods presented here. Specifically, studies which showed that edge or vertex histogram kernels were the least computationally expensive~\cite{sugiyama2018graphkernels}, while also being compatible with edge/vertex labels. This saved a lot of time which would have been spent searching for the graph kernel which would scale best to handle the large number of documents. Additionally, the graph kernel paper from Vishwanathan, and the survey paper form Kirege, allowed for clear understanding of the different types of graph kernel methods and their applications in a variety of fields, as well as pros and cons to each kernel.\\

Upon reviewing the literature surrounding each of these topics, it was clear there was much more work to do with these methods. Specifically, that the topic could use more published work on the use of graph kernels in conjunction with other machine learning methods, in this case hierarchical clustering. Also, this work provides two very different text datasets that can be used for additional studies on the application of these methods, as well as other natural language processing or text mining efforts. Most importantly, this work provides an end to end framework for text mining while using graphs for the R and the data science communities. These contributions will add to the existing body of work on graph kernels in the context of text mining and will lead to new development of using graph representation of text for machine learning.




%%%%%%%%%%






