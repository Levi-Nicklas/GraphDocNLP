---
title: "NHTSA Reports"
output: html_notebook
---


#### Purpose: This notebook will use scripts from the `Development/Scripts` folder and read data from the `Data/NHTSA` folders to analyze the NHTSA reports. The NHTSA reports are strating as just text from the "summary" portion of the reports.

# Initial Processing

```{r, warning = F, message = F, error = F}
# Libraries
library(tidyverse)
library(here)
library(igraph)
library(graphkernels)
library(tidytext)

# Data Import
file.names <- list.files(path = here::here("Data/NHTSA/RawData"))
papers <- list()

for(i in 1:length(file.names)){
  temp.file.name <- here::here(paste0("Data/NHTSA/RawData/", file.names[i]))

  if(grepl("paper_",temp.file.name)){
    temp.file <- readRDS(temp.file.name)
    
    temp.file <- paste(temp.file, collapse = ". ")
    
    papers[i] <- temp.file
  }
}
```

We read the papers into memory. Now lets put them together in a nice way... perhaps a dataframe.

```{r}
papers <- data.frame(id = 1:length(papers),
           text = unlist(papers))

#saveRDS(papers, file = here::here("Data/NHTSA/RawData/ConsolidatedPapers.rds")) # object too big for github.
```

Now we can clean the data. Tokenizing, removing stop words, stemming words, etc.

```{r, message = F, warning = F, error = F}
#papers <- readRDS(here::here("Data/NHTSA/RawData/ConsolidatedPapers.rds")) # onject too big for github.
source(here::here("Development/Scripts/df_to_graph_list.R"))

nhtsa_graphs <- df_to_graph_list(papers$text)

# cleaned <- papers %>% 
#   select(id,text) %>% 
#   tidytext::unnest_tokens(output = "words" ,
#                                   input = text, 
#                                   token = "skip_ngrams",
#                                   n = 2,
#                                   k = 2) %>%  
#       # filter to only 1 document.
#       dplyr::filter(id == 1) %>% 
#       # split bigram
#       tidyr::separate(words, c("word1", "word2"), sep = " ") %>%
#       # remove stop words
#       anti_join(stop_words, by = c("word1" = "word")) %>% 
#       anti_join(stop_words, by = c("word2" = "word")) %>% 
#       # toss NA values.
#       tidyr::drop_na() %>% 
#       # clean out punctuation %>% 
#       mutate(word1_toss = stringr::str_detect(word1, "\\.")) %>% 
#       mutate(word2_toss = stringr::str_detect(word2, "\\.")) %>% 
#       mutate(toss_pair = word1_toss|word2_toss) %>% 
#       filter(toss_pair == F) %>% 
#       # clean out numebrs
#       select(id,word1,word2) %>% 
#       mutate(word1_toss = stringr::str_detect(word1,"[:digit:]")) %>% 
#       mutate(word2_toss = stringr::str_detect(word2,"[:digit:]")) %>% 
#       mutate(toss_pair = word1_toss|word2_toss) %>% 
#       filter(toss_pair == F) %>% 
#       select(id,word1,word2) %>% 
#       # only keep words of > len 3
#       mutate(word1_toss = (nchar(word1)<4)) %>% 
#       mutate(word2_toss = (nchar(word2)<4)) %>% 
#       mutate(toss_pair = word1_toss|word2_toss) %>% 
#       filter(toss_pair == F) %>% 
#       select(id, word1, word2) %>% 
#       # group each bigram.
#       group_by(word1, word2) %>% 
#       # count occurances.
#       count() %>% 
#       # produce Graph.
#       igraph::graph_from_data_frame()
# 
# # Data frame of word cluster membership.
# cleaned_words_df <- cleaned %>% igraph::clusters()
# cleaned_words_df <- as.data.frame(cleaned_words_df$membership)
# cleaned_words_df$words <- rownames(cleaned_words_df)
# colnames(cleaned_words_df) <- c("member", "words")
# reduced_clusters <- cleaned_words_df %>% 
#   filter(member != 1)
# 
# # Get single largest cluster.
# big_graph <- cleaned %>% 
#   as_edgelist() %>% 
#   as.data.frame() %>% 
#   anti_join(y = reduced_clusters, by = c("V1" = "words")) %>% 
#   anti_join(y = reduced_clusters, by = c("V2" = "words")) %>% 
#   graph_from_data_frame() %>% plot()
# 



```

Let's explore the object.

```{r}
length(nhtsa_graphs)

nhtsa_graphs[[1]][[1]] %>% is.igraph()
```

Okay, the object looks like what we want now. Lets compute a graph kernel.

```{r, warning = F, message = F}
source(here::here("Development/Scripts/compute_graph_similarity.R"))
nhtsa_kernel <- matrix(rep(0,48*48), nrow = 48)



for(i in 1:length(nhtsa_graphs)){
  nhtsa_kernel[i,] <- compute_graph_similarity(nhtsa_graphs[[i]])
}
```

Awesome. Im going to try clustering with this real quick.
# TRY SCALING

```{r, include = F, eval = F}
library(caret)

scaled_kernel <- scale(nhtsa_kernel, center = T, scale = T)

scaled_cluster <- dist(scaled_kernel, method = "manhattan", p = 4) %>% 
  hclust(method = "ward.D")

scaled_cluster %>%
  plot()

papers$k3 <- scaled_cluster %>% 
  cutree(k = 3)

papers$k5 <- scaled_cluster %>% 
  cutree(k = 5)

papers$k7 <- scaled_cluster %>% 
  cutree(k = 7)

papers %>% 
  select(id,k3,k5,k7)

# viz

set.seed(23)

scaled_pts <- dist(scaled_kernel, method = "manhattan") %>% 
  MASS::isoMDS()



papers$x <- scaled_pts$points[,1]
papers$y <- scaled_pts$points[,2]
#papers$x <- pca_kernel$x[,1]
#papers$y <- pca_kernel$x[,2]

test_kmeans <- papers %>% 
  select(x,y) %>% 
  kmeans(2)

papers$kmeans <- test_kmeans$cluster

papers %>%
  as.data.frame() %>% 
  ggplot(aes(x = x, y = y, color = as.factor(k3)))+
  geom_point() +
  ggtitle("RBF space")

papers %>%  
  as.data.frame() %>% 
  mutate(theta = atan(y/x),
         r = sqrt(x^2 + y^2)) %>% 
  ggplot(aes(x = r, y = theta, color = as.factor(k3)))+
  geom_point() +
  ggtitle("polar transform")

```

# TRY PCA
```{r}
#dendro1 <- dist(nhtsa_kernel, method = "manhattan") %>% 
#  hclust(method = "ward.D2")

## PCA

# try inverting kernel
#nhtsa_kernel_inv <- 1/nhtsa_kernel
nhtsa_kernel <- 1/nhtsa_kernel

pca_kernel <- nhtsa_kernel %>% prcomp(scale. = T, center = T)
#pca_kernel_inv <- nhtsa_kernel_inv %>% prcomp(scale. = T, center = T)



# PCA analysis.
dendro_pca <- dist(pca_kernel$rotation, method = "manhattan") %>% 
  hclust(method = "ward.D")

dendro_pca %>%
  plot()

papers$k3 <- dendro_pca %>% 
  cutree(k = 3)

papers$k5 <- dendro_pca %>% 
  cutree(k = 5)

papers$k7 <- dendro_pca %>% 
  cutree(k = 7)

papers %>% 
  select(id,k3,k5,k7)
```

```{r}
set.seed(23)

#nhtsa_pts <- dist(pca_kernel$rotation, method = "manhattan") %>% 
#  MASS::isoMDS()

#papers$x <- nhtsa_pts$points[,1]
#papers$y <- nhtsa_pts$points[,2]
papers$x <- pca_kernel$x[,1]
papers$y <- pca_kernel$x[,2]

test_kmeans <- papers %>% 
  select(x,y) %>% 
  kmeans(3)

papers$kmeans <- test_kmeans$cluster

library(factoextra)

papers %>%
  as.data.frame() %>% 
  ggplot(aes(x = x, y = y, color = as.factor(k3)))+
  geom_point() + 
  ggtitle("rbf space")

papers %>%
  as.data.frame() %>% 
  filter(x < 2) %>% 
  filter(y < 2) %>% 
  ggplot(aes(x = x, y = y, color = as.factor(k3)))+
  geom_point() + 
  ggtitle("rbf space - filtered")

# papers %>%
#   as.data.frame() %>% 
#   mutate(theta = atan(y/x),
#          r = sqrt(x^2 + y^2)) %>% 
#   ggplot(aes(x = r, y = theta, color = as.factor(k3)))+
#   geom_point() +
#   ggtitle("polar transform")

fviz_pca_ind(pca_kernel, label="none", habillage = papers$k3,
             addEllipses=F, ellipse.level=0.95)
fviz_cluster()

fviz_pca_var(pca_kernel)
fviz_pca_biplot(pca_kernel, label ="ind")
summary(pca_kernel)
```


# WIP
```{r}
nhtsa_graphs_only <- list()

for(i in 1:length(nhtsa_graphs)){
  nhtsa_graphs_only[[i]] <- nhtsa_graphs[[i]][[1]]
}

graphkernel_directcompute <- graphkernels::CalculateEdgeHistKernel(nhtsa_graphs_only)

## Try CalculateEdgeHistGaus
#fix the graph labels
source(here::here("Development/Scripts/correcting_vertex_labels.R"))
nhtsa_graphs_only[[1]] %>% igraph::vertex.attributes()
```