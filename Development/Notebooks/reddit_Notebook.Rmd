---
title: "reddit_Notebook"
author: "Levi C. Nicklas"
date: "4/7/2021"
output: html_document
---

```{r setup, warning = FALSE, message=FALSE}
# Libraries
library(tidyverse)
library(here)
library(igraph)
library(graphkernels)
library(tidytext)
my_palette <-c("#ff333a","#f66025","#ffca3a","#8ac926","#1982c4","#6a4c93","#4b1592")

# Data Import
file.names <- list.files(path = here::here("Data/RawData"))

threads <- list()

for(i in 1:length(file.names)){
  temp.file.name <- here::here(paste0("Data/RawData/", file.names[i]))

  if(grepl("subreddit-",temp.file.name)){
    temp.file <- readRDS(temp.file.name)
    threads[[i]] <- temp.file
  }
}
```

Reformat the data to be text blobs _by thread_; observational unit in this analysis is now a **thread**.

```{r}
### Compress Thread text per post. ###

thread_text_list <- list()

for(j in 1:length(threads)){
  thread_df <- threads[[j]] 

  thread_df <- thread_df %>% 
  # Get top level of thread.
  mutate(thread_id = str_extract(structure,pattern = "[:digit:]{1,2}")) %>% 
  # Make Composite key of thread_ID and 
  mutate(post_thread_id = paste0(thread_id,"_x_",title)) 

  thread_df_compressed <- data.frame(post_thread_id = c(),
                                  text = c(),
                                  URL = c())
  post_threads <- unique(thread_df$post_thread_id)

  for(i in 1:length(post_threads)){
    thread_text <- thread_df %>% 
      filter(post_thread_id == post_threads[i]) %>% 
      select(comment) 
    thread_post <- thread_df %>% 
      filter(post_thread_id == post_threads[i]) %>% 
      select(post_text) %>% 
      unique() %>% 
      pull()
    thread_url <- thread_df %>% 
      filter(post_thread_id == post_threads[i]) %>% 
      select(URL) %>% 
      unique() %>% 
      pull()
    
    # Collapse Post and Comments into one blob.
    thread_text <- paste(thread_post, thread_text, collapse = ".")
    
    tmp_df <- data.frame(post_thread_id = c(post_threads[i]),
                                    text = c(thread_text),
                                    URL = c(thread_url))
    
    thread_df_compressed <- rbind(thread_df_compressed, tmp_df)
    #print(paste0("On Post ",j,", thread ",i))
  }
  thread_text_list[[j]] <- thread_df_compressed
  
}

rm(thread_df, temp.file, thread_df_compressed)
```


```{r}
# Make one big dataframe.

post_thread_text <- thread_text_list[[1]]
post_thread_text$queryword <- 1

for(i in 2:length(thread_text_list)){
  tmp_df <- thread_text_list[[i]]
  tmp_df$queryword <- i
  post_thread_text <- rbind(post_thread_text,tmp_df)
}

rm(threads, tmp_df)
```


## DATA CLEANING WITH EXISTING FUNCTIONS ##

Use `{furrr}` to make this computable in a reasonable amount of time.
```{r, warning = F, error = F, message = F}
source(here::here("Development/Scripts/df_to_graph_list.R"))
source(here::here("Development/Scripts/convert_to_graphs.R"))
#thread_graphs <- df_to_graph_list(post_thread_text$text)

# Will sample out 1000.
set.seed(23)
post_thread_text_sample <- sample_n(post_thread_text, 1000, replace = F)

thread_graphs <- convert_to_graphs()

#Takes a little bit, be patient.
```

## COMPUTE GRAPH KERNELS ##

```{r, message = F}
source(here::here("Development/Scripts/compute_graph_similarity.R"))
source(here::here("Development/Scripts/compute_kernel.R"))
#reddit_thread_kernel <- compute_kernel()

# This will take like 1 hr to run.
#saveRDS(reddit_thread_kernel, here::here("Data/ProcessedData/RedditThreadKerenel1000.RDS"))

reddit_thread_kernel <- readRDS(here::here("Data/ProcessedData/RedditThreadKerenel1000.RDS"))
```

Make one matrix

```{r}
reddit_thread_kernel_matrix <- matrix(data = rep(0, 1000*1000), nrow = 1000)

for(i in 1:length(reddit_thread_kernel)){
  reddit_thread_kernel_matrix[i,] <- reddit_thread_kernel[[i]]
}

rm(reddit_thread_kernel)
```

# Hierarchical Cluster

```{r}
# Any NA values?
thread_boolean <- reddit_thread_kernel_matrix[1,] %>% is.na() 

# Remove bad text threads -> NA's
post_thread_text_sample <- post_thread_text_sample[!thread_boolean,]

# Resize
reddit_thread_kernel_matrix <- reddit_thread_kernel_matrix[!thread_boolean,]
reddit_thread_kernel_matrix <- reddit_thread_kernel_matrix[,!thread_boolean]

#invert Matrix (Sim - > Dist)
reddit_thread_kernel_matrix <- 1/reddit_thread_kernel_matrix

library(caret)

pca_kernel <- reddit_thread_kernel_matrix %>% prcomp(scale. = T, center = T)

dendro_pca <- dist(pca_kernel$rotation, method = "manhattan") %>% 
  hclust(method = "ward.D")

dendro_pca %>% plot()

reddit_thread_kernel_matrix$k3 <- dendro_pca %>% 
  cutree(k = 3)

reddit_thread_kernel_matrix$k5 <- dendro_pca %>% 
  cutree(k = 5)

reddit_thread_kernel_matrix$k7 <- dendro_pca %>% 
  cutree(k = 7)

```

```{r, message=F}
library(factoextra)

reddit_dendo <- fviz_dend(dendro_pca, k = 3, cex = 0.5,  show_labels = F, k_colors = my_palette) +
  labs(x = "Document ID",
       y = "Height",
       title = "reddit Thread Documents - HClust",
       subtitle = "Manhattan Distance & Ward.D Linkage")

ggsave(reddit_dendo, filename = here::here("Thesis_Tex/Content/Images/reddit_dendo.png"))
```

```{r}
fviz_pca_ind(pca_kernel, label="none", habillage = post_thread_text_sample$k3,
             addEllipses=F, ellipse.level=0.95)
```


```{r}
fviz_pca_biplot(pca_kernel, label ="ind")
```

```{r}


```


# Kernel Density Estimate Clustering

